{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b18b567-4d05-4677-af4d-69773ad0fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from datasets.features import Features, Value, Sequence\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Unified data loading\n",
    "def load_data(csv_path: str) -> tuple[List[str], List[List[float]], int, List[str]]:\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding=encoding, delimiter=';', quotechar='\"', on_bad_lines='warn')\n",
    "            if not df.empty:\n",
    "                break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    else:\n",
    "        raise UnicodeDecodeError(f\"Failed to decode {csv_path} with tried encodings: {encodings}\")\n",
    "    \n",
    "    text_column = 'Text'\n",
    "    all_columns = df.columns.tolist()\n",
    "    if 'SANTA_ID' in all_columns:\n",
    "        ids = df['SANTA_ID'].tolist()\n",
    "        all_columns.remove('SANTA_ID')\n",
    "    else:\n",
    "        ids = [f\"ID_{i}\" for i in range(len(df))]\n",
    "    if text_column in all_columns:\n",
    "        all_columns.remove(text_column)\n",
    "    \n",
    "    # Remove the last 3 columns to match 11 labels\n",
    "    label_columns = all_columns[:-3]\n",
    "    \n",
    "    # Regularize label columns\n",
    "    df[label_columns] = df[label_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    \n",
    "    texts = df[text_column].tolist()\n",
    "    labels = df[label_columns].values.astype(float).tolist()\n",
    "    return texts, labels, len(label_columns), ids\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = (pred.predictions > 0.5).astype(float)\n",
    "    f1 = f1_score(labels, preds, average='micro')\n",
    "    roc_auc = roc_auc_score(labels, pred.predictions, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'f1': f1, 'roc_auc': roc_auc, 'accuracy': acc}\n",
    "\n",
    "# Function to perform k-fold training and evaluation\n",
    "def train_with_kfold(model_name: str, folder_path: str, output_dir: str, k: int = 5, epochs: int = 20, batch_size: int = 16) -> Dict:\n",
    "    all_texts, all_labels, num_labels, all_ids = [], [], 0, []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_path = os.path.join(folder_path, filename)\n",
    "            texts, labels, n_labels, ids = load_data(csv_path)\n",
    "            all_texts.extend(texts)\n",
    "            all_labels.extend(labels)\n",
    "            all_ids.extend(ids)\n",
    "            num_labels = n_labels  # Assume consistent number of labels\n",
    "    \n",
    "    if len(all_texts) < k:\n",
    "        raise ValueError(f\"Insufficient samples ({len(all_texts)}) for {k}-fold cross-validation\")\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(all_texts)):\n",
    "        print(f\"Training {model_name} fold {fold + 1}/{k}...\")\n",
    "        train_texts = [all_texts[i] for i in train_idx]\n",
    "        test_texts = [all_texts[i] for i in test_idx]\n",
    "        train_labels = [all_labels[i] for i in train_idx]\n",
    "        test_labels = [all_labels[i] for i in test_idx]\n",
    "        train_ids = [all_ids[i] for i in train_idx]\n",
    "        test_ids = [all_ids[i] for i in test_idx]\n",
    "        \n",
    "        features = Features({'text': Value('string'), 'labels': Sequence(Value('float32'))})\n",
    "        train_dataset_dict = {'text': train_texts, 'labels': train_labels}\n",
    "        test_dataset_dict = {'text': test_texts, 'labels': test_labels}\n",
    "        \n",
    "        train_dataset = Dataset.from_dict(train_dataset_dict, features=features)\n",
    "        test_dataset = Dataset.from_dict(test_dataset_dict, features=features)\n",
    "        \n",
    "        tokenizer = RobertaTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
    "        train_dataset = train_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "        test_dataset = test_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "        \n",
    "        train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            'FacebookAI/roberta-base', num_labels=num_labels, problem_type='multi_label_classification'\n",
    "        )\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{output_dir}/{model_name}_fold_{fold + 1}\",\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "            eval_strategy='steps',\n",
    "            eval_steps=100,\n",
    "            save_strategy='steps',\n",
    "            save_steps=100,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='f1',\n",
    "            seed=SEED\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        fold_results.append(eval_results)\n",
    "        trainer.save_model(f\"{output_dir}/{model_name}_fold_{fold + 1}\")\n",
    "        tokenizer.save_pretrained(f\"{output_dir}/{model_name}_fold_{fold + 1}\")\n",
    "    \n",
    "    avg_results = {k: np.mean([r[k] for r in fold_results]) for k in fold_results[0].keys() if k not in ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second']}\n",
    "    return avg_results\n",
    "\n",
    "def compare_models():\n",
    "    models = {\n",
    "        'Santa': 'SANTA', #Specify paths\n",
    "        'Mystery': 'MD',\n",
    "        'Combined': 'Combination'\n",
    "    }\n",
    "    output_dir = './fine_tuned_models'\n",
    "    results = {}\n",
    "    \n",
    "    # Perform k-fold for each model separately\n",
    "    for model_name, folder_path in models.items():\n",
    "        results[model_name] = train_with_kfold(model_name, folder_path, output_dir, k=5, epochs=20, batch_size=16)\n",
    "    \n",
    "    # Summary table\n",
    "    summary_df = pd.DataFrame(results).T.round(4)\n",
    "    print(\"\\nOverall Metrics Summary:\")\n",
    "    print(summary_df)\n",
    "    summary_df.to_csv('comparison_summary.csv', index=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
