{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d355b95-0f53-41ca-9880-aa5e9cb7ad39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cead47efaae8422992db565ea1b1af76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854c8000149f45f7b8f2144b7855f531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 01:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589732</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592411</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Santa fine-tuned and evaluated, saved to ./fine_tuned_models/Santa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790f3c18c8b44e0e992259c5769cd3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86453d13633d4dab826cb82d3f40838b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 01:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.704055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277679</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.703550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.702659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304018</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mystery fine-tuned and evaluated, saved to ./fine_tuned_models/Mystery\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b2b08a8eb4414b8c430638bc0f76f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73f21e16cad4d809845d18da676c7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 01:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.704055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277679</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.703550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.702659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304018</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Combined fine-tuned and evaluated, saved to ./fine_tuned_models/Combined\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'text' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 437\u001b[39m\n\u001b[32m    434\u001b[39m     summary_df.to_csv(\u001b[33m'\u001b[39m\u001b[33mcomparison_summary.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     compare_models()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 423\u001b[39m, in \u001b[36mcompare_models\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    418\u001b[39m     results[model_name] = train_and_evaluate_model(\n\u001b[32m    419\u001b[39m         model_name, folder_path, output_dir, train_texts, train_labels, test_texts, test_labels\n\u001b[32m    420\u001b[39m     )\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# Get Ollama predictions on the test set\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m ollama_preds = get_ollama_predictions(test_texts, test_ids, num_labels)\n\u001b[32m    424\u001b[39m true_labels = np.array(test_labels)\n\u001b[32m    425\u001b[39m ollama_f1 = f1_score(true_labels, ollama_preds, average=\u001b[33m'\u001b[39m\u001b[33mmicro\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 364\u001b[39m, in \u001b[36mget_ollama_predictions\u001b[39m\u001b[34m(test_texts, test_ids, num_labels)\u001b[39m\n\u001b[32m    136\u001b[39m     examples = [\n\u001b[32m    137\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"ACI08;\"Mr. Hosmer Angel came to the house again and proposed that we \u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03mshould marry before father came back. He was in dreadful earnest \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    338\u001b[39m \u001b[33;03mI will tell you all.';0;0;0;0;0;0;1;0;0;0;none;0;0\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m     ]\n\u001b[32m    340\u001b[39m     examples_str = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(examples)\n\u001b[32m    342\u001b[39m     prompt_template = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[33mYou are an expert in narratology, annotating literary texts based on the modular guidelines from Heyns and Van Zaanen (2024) for mystery novels (whodunits). For each text segment, output a CSV row with these columns: SANTA_ID;Text;Scene;Summary;Descriptive_passage;Analepsis;Prolepsis;Extradiegetic;Intradiegetic;Metadiegetic;Focalization;Voice_homodiegetic;Voice_heterodiegetic.\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[33mHere are the definitions for each tag to guide your annotation:\u001b[39m\n\u001b[32m    345\u001b[39m \n\u001b[32m    346\u001b[39m \u001b[33mScene: A segment of narrative discourse that presents the histoire (story), typically involving a coherent sequence of events with specific characters, time, and place, annotated using the SCENE tag.\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[33mSummary: A non-scene where events are condensed or narrated briefly, often as a sub-scene within a broader scene, assigned as a property of NON-SCENE.\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[33mDescriptive_passage: A non-scene focused on description rather than events, providing detailed pauses in the narrative for setting or character details, assigned as a property of NON-SCENE.\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[33mAnalepsis: A flashback that shifts the narrative time from the present to the past, tagged as ANALEPSIS, which can be embedded or interruptive.\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[33mProlepsis: A flash-forward occurring in forms like visions, prophecies, or foreshadowing, shifting narrative time to the future, tagged as PROLEPSIS, which can be embedded or interruptive.\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[33mExtradiegetic: The level of the narrator or implied author outside the story world, annotated with NARRATOR and value 0, potentially including metatextuality (meta).\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[33mIntradiegetic: The diegetic level of characters and events within the story, annotated with value 1 (and letters like 1a, 1b for sequential arrangement).\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[33mMetadiegetic: A secondary narrative embedded within the primary diegetic level, such as stories told by characters, annotated with value 2 (and letters for arrangement).\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[33mFocalization: The perspective from which the narrative is seen, indicating the narrator\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms access to information; can be zero/unrestricted (omniscient, knows more than characters), internal (limited to a character\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms knowledge), or external (observes without internal access); tag with FOCALIZATION and properties like EMBEDDED or INTERRUPTIVE.\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[33mVoice_homodiegetic: When the narrator appears in the story as a character, usually referring to themselves in the first person.\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[33mVoice_heterodiegetic: When the narrator does not appear in the story, with narration mostly in the third person.\u001b[39m\n\u001b[32m    357\u001b[39m \n\u001b[32m    358\u001b[39m \u001b[33mUse 1 for \u001b[39m\u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and 0 for \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in binary columns (e.g., Scene). \u001b[39m\n\u001b[32m    359\u001b[39m \u001b[33mExamples:\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mexamples_str\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    361\u001b[39m \n\u001b[32m    362\u001b[39m \u001b[33mNow annotate this new segment:\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[33mID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[33mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33mOutput only the CSV row (no extra text).\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    368\u001b[39m     annotations = []\n\u001b[32m    369\u001b[39m     start_time = time.time()\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'text' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from datasets.features import Features, Value, Sequence\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from typing import List, Dict\n",
    "import ollama\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Unified data loading (handles all models and returns ids)\n",
    "def load_data(csv_path: str) -> tuple[List[str], List[List[float]], int, List[str]]:\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding=encoding, delimiter=';', quotechar='\"', on_bad_lines='warn')\n",
    "            if not df.empty:\n",
    "                break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    else:\n",
    "        raise UnicodeDecodeError(f\"Failed to decode {csv_path} with tried encodings: {encodings}\")\n",
    "    \n",
    "    text_column = 'Text'\n",
    "    all_columns = df.columns.tolist()\n",
    "    if 'SANTA_ID' in all_columns:\n",
    "        ids = df['SANTA_ID'].tolist()\n",
    "        all_columns.remove('SANTA_ID')\n",
    "    else:\n",
    "        ids = [f\"ID_{i}\" for i in range(len(df))]\n",
    "    if text_column in all_columns:\n",
    "        all_columns.remove(text_column)\n",
    "    \n",
    "    # Handle 'Acts' column if present (for Santa)\n",
    "    acts_column = 'Acts' if 'Acts' in all_columns else None\n",
    "    numeric_label_columns = [col for col in all_columns if col != acts_column]\n",
    "    \n",
    "    # Regularize numeric label columns: convert to numeric, coerce errors to NaN, fill NaN with 0\n",
    "    df[numeric_label_columns] = df[numeric_label_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    \n",
    "    # One-hot encode the 'Acts' column\n",
    "    if acts_column:\n",
    "        acts_encoded = pd.get_dummies(df[acts_column], prefix='Acts')\n",
    "        df = pd.concat([df, acts_encoded], axis=1)\n",
    "        label_columns = numeric_label_columns + acts_encoded.columns.tolist()\n",
    "    else:\n",
    "        # For Mystery and Combined, take first 11 or all except last 3\n",
    "        label_columns = all_columns[:11] if len(all_columns) <= 11 else all_columns[:-3]\n",
    "    \n",
    "    texts = df[text_column].tolist()\n",
    "    labels = df[label_columns].values.astype(float).tolist()\n",
    "    return texts, labels, len(label_columns), ids\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = (pred.predictions > 0.5).astype(float)\n",
    "    f1 = f1_score(labels, preds, average='micro')\n",
    "    roc_auc = roc_auc_score(labels, pred.predictions, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'f1': f1, 'roc_auc': roc_auc, 'accuracy': acc}\n",
    "\n",
    "# Function to train and evaluate a RoBERTa model\n",
    "def train_and_evaluate_model(model_name: str, folder_path: str, output_dir: str, train_texts: List[str], train_labels: List[List[float]], \n",
    "                            test_texts: List[str], test_labels: List[List[float]], epochs: int = 3, batch_size: int = 16) -> Dict:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
    "    \n",
    "    # Prepare datasets\n",
    "    features = Features({'text': Value('string'), 'labels': Sequence(Value('float32'))})\n",
    "    train_dataset_dict = {'text': train_texts, 'labels': train_labels}\n",
    "    test_dataset_dict = {'text': test_texts, 'labels': test_labels}\n",
    "    \n",
    "    train_dataset = Dataset.from_dict(train_dataset_dict, features=features)\n",
    "    test_dataset = Dataset.from_dict(test_dataset_dict, features=features)\n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "    test_dataset = test_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "    \n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'FacebookAI/roberta-base', num_labels=len(train_labels[0]), problem_type='multi_label_classification'\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{output_dir}/{model_name}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',\n",
    "        seed=SEED\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    trainer.save_model(f\"{output_dir}/{model_name}\")\n",
    "    tokenizer.save_pretrained(f\"{output_dir}/{model_name}\")\n",
    "    \n",
    "    print(f\"Model {model_name} fine-tuned and evaluated, saved to {output_dir}/{model_name}\")\n",
    "    return eval_results\n",
    "\n",
    "# Ollama few-shot prediction function\n",
    "def get_ollama_predictions(test_texts: List[str], test_ids: List[str], num_labels: int) -> np.ndarray:\n",
    "    examples = [\n",
    "        \"\"\"ACI08;\"Mr. Hosmer Angel came to the house again and proposed that we \n",
    "should marry before father came back. He was in dreadful earnest \n",
    "and made me swear, with my hands on the Testament, that whatever \n",
    "happened I would always be true to him. Mother said he was quite \n",
    "right to make me swear, and that it was a sign of his passion. \n",
    "Mother was all in his favour from the first and was even fonder \n",
    "of him than I was. Then, when they talked of marrying within the \n",
    "week, I began to ask about father; but they both said never to \n",
    "mind about father, but just to tell him afterwards, and mother \n",
    "said she would make it all right with him.\";0;0;0;1;0;1;1;0;0;0;none;0;0\"\"\",\n",
    "        \"\"\"ACI05;\"I had had so many reasons to believe in my friend's subtle powers \n",
    "of reasoning and extraordinary energy in action that I felt that \n",
    "he must have some solid grounds for the assured and easy \n",
    "demeanour with which he treated the singular mystery which he had \n",
    "been called upon to fathom. Once only had I known him to fail, in \n",
    "the case of the King of Bohemia and of the Irene Adler \n",
    "photograph; but when I looked back to the weird business of the \n",
    "Sign of Four, and the extraordinary circumstances connected with \n",
    "the Study in Scarlet, I felt that it would be a strange tangle \n",
    "indeed which he could not unravel. \n",
    "I left him then, still puffing at his black clay pipe, with the \n",
    "conviction that when I came again on the next evening I would \n",
    "find that he held in his hands all the clues which would lead up \n",
    "to the identity of the disappearing bridegroom of Miss Mary \n",
    "Sutherland. \n",
    "A professional case of great gravity was engaging my own \n",
    "attention at the time, and the whole of next day I was busy at \n",
    "the bedside of the sufferer. It was not until close upon six \n",
    "o'clock that I found myself free and was able to spring into a \n",
    "hansom and drive to Baker Street, half afraid that I might be too \n",
    "late to assist at the dénouement of the little mystery. I found \n",
    "Sherlock Holmes alone, however, half asleep, with his long, thin \n",
    "form curled up in the recesses of his armchair. A formidable \n",
    "array of bottles and test-tubes, with the pungent cleanly smell \n",
    "of hydrochloric acid, told me that he had spent his day in the \n",
    "chemical work which was so dear to him.\";0;1;0;0;0;1;1;1;0;0;none;0;0\"\"\",\n",
    "        \"\"\"TAIN27;\"On the previous morning, two gentlemen had called to see his \n",
    "master. They were Italians, and the elder of the two, a man of about \n",
    "forty, gave his name as Signor Ascanio. \n",
    "The younger was a well-dressed lad of about twenty-four. \n",
    "Count Foscatini was evidently prepared for their visit and \n",
    "immediately sent Graves out upon some trivial errand. \n",
    "Here the man paused and hesitated in his story. \n",
    "In the end, however, he admitted that, curious as to the purport \n",
    "of the interview, he had not obeyed immediately, but had lingered \n",
    "about endeavouring to hear something of what was going on. \n",
    "The conversation was carried on in so low a tone that he was not \n",
    "as successful as he had hoped; but he gathered enough to make it \n",
    "clear that some kind of monetary proposition was being discussed, \n",
    "and that the basis of it was a threat. \n",
    "The discussion was anything but amicable. \n",
    "In the end, Count Foscatini raised his voice slightly, and the \n",
    "listener heard these words clearly:  'I have no time to argue \n",
    "further now, gentlemen. \n",
    "If you will dine with me to-morrow night at eight o’clock, we will\n",
    " resume the discussion.'  Afraid of being discovered listening, \n",
    "Graves had then hurried out to do his master’s errand. \n",
    "This evening the two men had arrived punctually at eight. \n",
    "During dinner they had talked of indifferent matters—politics, the\n",
    " weather, and the theatrical world. \n",
    "When Graves had placed the port upon the table and brought in the \n",
    "coffee his master told him that he might have the evening off. \n",
    "'Was that a usual proceeding of his when he had guests?' asked the\n",
    " inspector. \n",
    "'No, sir; it wasn’t. \n",
    "That’s what made me think it must be some business of a very \n",
    "unusual kind that he was going to discuss with these gentlemen.'  \n",
    "That finished Graves’s story. \n",
    "He had gone out about 8.30, and, meeting a friend, had accompanied\n",
    " him to the Metropolitan Music Hall in Edgware Road. \n",
    "Nobody had seen the two men leave, but the time of the murder was \n",
    "fixed clearly enough at 8.47. \n",
    "A small clock on the writing-table had been swept off by \n",
    "Foscatini’s arm, and had stopped at that hour, which agreed with \n",
    "Miss Rider’s telephone summons. \n",
    "The police surgeon had made his examination of the body, and it \n",
    "was now lying on the couch. \n",
    "I saw the face for the first time—the olive complexion, the long \n",
    "nose, the luxuriant black moustache, and the full red lips drawn \n",
    "back from the dazzlingly white teeth. \n",
    "Not altogether a pleasant face. \n",
    "'Well,' said the inspector, refastening his notebook. \n",
    "'The case seems clear enough. \n",
    "The only difficulty will be to lay our hands on this Signor \n",
    "Ascanio. \n",
    "I suppose his address is not in the dead man’s pocket-book by any \n",
    "chance?'  As Poirot had said, the late Foscatini was an orderly \n",
    "man. \n",
    "Neatly written in small, precise handwriting was the inscription, \n",
    "'Signor Paolo Ascanio, Grosvenor Hotel.'  The inspector busied \n",
    "himself with the telephone, then turned to us with a grin. \n",
    "'Just in time. \n",
    "Our fine gentleman was off to catch the boat train to the \n",
    "Continong. \n",
    "Well, gentlemen, that’s about all we can do here. \n",
    "It’s a bad business, but straightforward enough. \n",
    "One of these Italian vendetta things, as likely as not.'  Thus \n",
    "airily dismissed, we found our way downstairs. \n",
    "Dr. Hawker was full of excitement. \n",
    "'Like the beginning of a novel, eh? \n",
    "Real exciting stuff. \n",
    "Wouldn’t believe it if you read about it.'  Poirot did not speak. \n",
    "He was very thoughtful. \n",
    "All the evening he had hardly opened his lips. \n",
    "'What says the master detective, eh?' asked Hawker, clapping him \n",
    "on the back. \n",
    "'Nothing to work your grey cells over this time.'  'You think \n",
    "not?'  'What could there be?'  'Well, for example, there is the \n",
    "window.'  'The window? \n",
    "But it was fastened. \n",
    "Nobody could have got out or in that way. \n",
    "I noticed it specially.'  'And why were you able to notice it?'  \n",
    "The doctor looked puzzled. \n",
    "Poirot hastened to explain. \n",
    "'It is to the curtains I refer. \n",
    "They were not drawn. \n",
    "A little odd, that. \n",
    "And then there was the coffee. \n",
    "It was very black coffee.'  'Well, what of it?'  'Very black,' \n",
    "repeated Poirot. \n",
    "'In conjunction with that let us remember that very little of the \n",
    "rice soufflé was eaten, and we get—what?'  'Moonshine,' laughed \n",
    "the doctor. \n",
    "'You’re pulling my leg.'  'Never do I pull the leg. \n",
    "Hastings here knows that I am perfectly serious.'  'I don’t know \n",
    "what you are getting at, all the same,' I confessed. \n",
    "'You don’t suspect the manservant, do you? \n",
    "He might have been in with the gang, and put some dope in the \n",
    "coffee. \n",
    "I suppose they’ll test his alibi?'  'Without doubt, my friend; but\n",
    " it is the alibi of Signor Ascanio that interests me.'  'You think\n",
    " he has an alibi?'  'That is just what worries me. \n",
    "I have no doubt that we shall soon be enlightened on that point.'\n",
    " The _Daily Newsmonger_ enabled us to become conversant with \n",
    "succeeding events. \n",
    "Signor Ascanio was arrested and charged with the murder of Count \n",
    "Foscatini. \n",
    "When arrested, he denied knowing the Count, and declared he had \n",
    "never been near Regent’s Court either on the evening of the crime \n",
    "or on the previous morning. \n",
    "The younger man had disappeared entirely. \n",
    "Signor Ascanio had arrived alone at the Grosvenor Hotel from the \n",
    "Continent two days before the murder. \n",
    "All efforts to trace the second man failed. \n",
    "Ascanio, however, was not sent for trial. \n",
    "No less a personage than the Italian Ambassador himself came \n",
    "forward and testified at the police-court proceedings that Ascanio\n",
    " had been with him at the Embassy from eight till nine that \n",
    "evening. \n",
    "The prisoner was discharged. \n",
    "Naturally, a lot of people thought that the crime was a political \n",
    "one, and was being deliberately hushed up. \n",
    "Poirot had taken a keen interest in all these points. \n",
    "Nevertheless, I was somewhat surprised when he suddenly informed \n",
    "me one morning that he was expecting a visitor at eleven o’clock, \n",
    "and that that visitor was none other than Ascanio himself. \n",
    "'He wishes to consult you?'  '_Du tout_, Hastings. \n",
    "I wish to consult him.'  'What about?'  'The Regent’s Court \n",
    "murder.'  'You are going to prove that he did it?'  'A man cannot \n",
    "be tried twice for murder, Hastings. \n",
    "Endeavour to have the common sense. \n",
    "Ah, that is our friend’s ring. A few minutes later Signor \n",
    "Ascanio was ushered in—a small, thin man with a secretive and \n",
    "furtive glance in his eyes. \n",
    "He remained standing, darting suspicious glances from one to the \n",
    "other of us. \n",
    "'Monsieur Poirot?'  My little friend tapped himself gently on the \n",
    "chest. \n",
    "'Be seated, signor. \n",
    "You received my note. \n",
    "I am determined to get to the bottom of this mystery. \n",
    "In some small measure you can aid me. \n",
    "Let us commence. \n",
    "You—in company with a friend—visited the late Count Foscatini on \n",
    "the morning of Tuesday the 9th——'  The Italian made an angry \n",
    "gesture. \n",
    "'I did nothing of the sort. \n",
    "I have sworn in court——'  '_Précisément_—and I have a little idea \n",
    "that you have sworn falsely.'  'You threaten me? \n",
    "Bah! \n",
    "I have nothing to fear from you. \n",
    "I have been acquitted.'  'Exactly; and as I am not an imbecile, it\n",
    " is not with the gallows I threaten you—but with publicity. \n",
    "Publicity! \n",
    "I see that you do not like the word. \n",
    "I had an idea that you would not. \n",
    "My little ideas, you know, they are very valuable to me. \n",
    "Come, signor, your only chance is to be frank with me. \n",
    "I do not ask to know whose indiscretions brought you to England. \n",
    "I know this much, you came for the especial purpose of seeing \n",
    "Count Foscatini.'  'He was not a count,' growled the Italian. \n",
    "'I have already noted the fact that his name does not appear in \n",
    "the _Almanach de Gotha_. \n",
    "Never mind, the title of count is often useful in the profession \n",
    "of blackmailing.'  'I suppose I might as well be frank. \n",
    "You seem to know a good deal.'  'I have employed my grey cells to \n",
    "some advantage. \n",
    "Come, Signor Ascanio, you visited the dead man on the Tuesday \n",
    "morning—that is so, is it not?'  'Yes; but I never went there on \n",
    "the following evening. \n",
    "There was no need. \n",
    "I will tell you all.';0;0;0;0;0;0;1;0;0;0;none;0;0\"\"\"\n",
    "    ]\n",
    "    examples_str = \"\\n\".join(examples)\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "You are an expert in narratology, annotating literary texts based on the modular guidelines from Heyns and Van Zaanen (2024) for mystery novels (whodunits). For each text segment, output a CSV row with these columns: SANTA_ID;Text;Scene;Summary;Descriptive_passage;Analepsis;Prolepsis;Extradiegetic;Intradiegetic;Metadiegetic;Focalization;Voice_homodiegetic;Voice_heterodiegetic.\n",
    "Here are the definitions for each tag to guide your annotation:\n",
    "\n",
    "Scene: A segment of narrative discourse that presents the histoire (story), typically involving a coherent sequence of events with specific characters, time, and place, annotated using the SCENE tag.\n",
    "Summary: A non-scene where events are condensed or narrated briefly, often as a sub-scene within a broader scene, assigned as a property of NON-SCENE.\n",
    "Descriptive_passage: A non-scene focused on description rather than events, providing detailed pauses in the narrative for setting or character details, assigned as a property of NON-SCENE.\n",
    "Analepsis: A flashback that shifts the narrative time from the present to the past, tagged as ANALEPSIS, which can be embedded or interruptive.\n",
    "Prolepsis: A flash-forward occurring in forms like visions, prophecies, or foreshadowing, shifting narrative time to the future, tagged as PROLEPSIS, which can be embedded or interruptive.\n",
    "Extradiegetic: The level of the narrator or implied author outside the story world, annotated with NARRATOR and value 0, potentially including metatextuality (meta).\n",
    "Intradiegetic: The diegetic level of characters and events within the story, annotated with value 1 (and letters like 1a, 1b for sequential arrangement).\n",
    "Metadiegetic: A secondary narrative embedded within the primary diegetic level, such as stories told by characters, annotated with value 2 (and letters for arrangement).\n",
    "Focalization: The perspective from which the narrative is seen, indicating the narrator's access to information; can be zero/unrestricted (omniscient, knows more than characters), internal (limited to a character's knowledge), or external (observes without internal access); tag with FOCALIZATION and properties like EMBEDDED or INTERRUPTIVE.\n",
    "Voice_homodiegetic: When the narrator appears in the story as a character, usually referring to themselves in the first person.\n",
    "Voice_heterodiegetic: When the narrator does not appear in the story, with narration mostly in the third person.\n",
    "\n",
    "Use 1 for 'yes' and 0 for 'no' in binary columns (e.g., Scene). \n",
    "Examples:\n",
    "{examples_str}\n",
    "\n",
    "Now annotate this new segment:\n",
    "ID: {id}\n",
    "Text: {text}\n",
    "Output only the CSV row (no extra text).\n",
    "\"\"\"\n",
    "\n",
    "    annotations = []\n",
    "    start_time = time.time()\n",
    "    for i, (text, id_val) in enumerate(zip(test_texts, test_ids)):\n",
    "        print(f\"Processing row {i+1} of {len(test_texts)}...\")\n",
    "        prompt = prompt_template.format(id=id_val, text=text)\n",
    "        response = ollama.chat(model='llama3:8b', messages=[{'role': 'user', 'content': prompt}], options={'temperature': 0})\n",
    "        annotated_row = response['message']['content'].strip()\n",
    "        annotations.append(annotated_row)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Parse annotations into a structured format for metrics\n",
    "    pred_labels = []\n",
    "    for row in annotations:\n",
    "        # Split and convert to list of floats (skip SANTA_ID and Text)\n",
    "        parts = row.split(';')[2:]  # Skip first two columns\n",
    "        pred_labels.append([float(x) for x in parts])\n",
    "    return np.array(pred_labels)\n",
    "\n",
    "def compare_models():\n",
    "    models = {\n",
    "        'Santa': 'Finaal_data\\\\SANTA',\n",
    "        'Mystery': 'Finaal_data\\\\MD',\n",
    "        'Combined': 'Finaal_data\\\\Combination'\n",
    "    }\n",
    "    output_dir = './fine_tuned_models'\n",
    "    results = {}\n",
    "    \n",
    "    # Aggregate data from all folders for a single train-test split\n",
    "    all_texts, all_labels, num_labels, all_ids = [], [], 0, []\n",
    "    for model_name, folder_path in models.items():\n",
    "        first_csv = next((f for f in os.listdir(folder_path) if f.endswith('.csv')), None)\n",
    "        if not first_csv:\n",
    "            raise ValueError(f\"No CSV files found in {folder_path}\")\n",
    "        csv_path = os.path.join(folder_path, first_csv)\n",
    "        texts, labels, n_labels, ids = load_data(csv_path)\n",
    "        all_texts.extend(texts)\n",
    "        all_labels.extend(labels)\n",
    "        all_ids.extend(ids)\n",
    "        num_labels = n_labels  # Assume consistent number of labels\n",
    "    \n",
    "    # Single train-test split (80-20)\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        all_texts, all_labels, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    train_ids, test_ids = train_test_split(all_ids, test_size=0.2, random_state=SEED)\n",
    "    \n",
    "    # Train and evaluate each RoBERTa model\n",
    "    for model_name, folder_path in models.items():\n",
    "        results[model_name] = train_and_evaluate_model(\n",
    "            model_name, folder_path, output_dir, train_texts, train_labels, test_texts, test_labels\n",
    "        )\n",
    "    \n",
    "    # Get Ollama predictions on the test set\n",
    "    ollama_preds = get_ollama_predictions(test_texts, test_ids, num_labels)\n",
    "    true_labels = np.array(test_labels)\n",
    "    ollama_f1 = f1_score(true_labels, ollama_preds, average='micro')\n",
    "    ollama_roc_auc = roc_auc_score(true_labels, ollama_preds, average='micro')\n",
    "    ollama_acc = accuracy_score(true_labels, ollama_preds)\n",
    "    results['Ollama'] = {'f1': ollama_f1, 'roc_auc': ollama_roc_auc, 'accuracy': ollama_acc}\n",
    "    \n",
    "    # Summary table\n",
    "    summary_df = pd.DataFrame(results).T.round(4)\n",
    "    print(\"\\nOverall Metrics Summary:\")\n",
    "    print(summary_df)\n",
    "    summary_df.to_csv('comparison_summary.csv', index=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "906833e1-fb55-4638-81c9-1de3fd9dee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Santa fold 1/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93baa15ecf74abaab6664838328c80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0092ff01e837412b97b3d3134c311024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1700' max='2360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1700/2360 3:17:41 < 1:16:50, 0.14 it/s, Epoch 14/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.415500</td>\n",
       "      <td>0.409222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797435</td>\n",
       "      <td>0.093817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.318321</td>\n",
       "      <td>0.544000</td>\n",
       "      <td>0.892455</td>\n",
       "      <td>0.373134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.323426</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.883699</td>\n",
       "      <td>0.381663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.303212</td>\n",
       "      <td>0.618045</td>\n",
       "      <td>0.911246</td>\n",
       "      <td>0.434968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.310326</td>\n",
       "      <td>0.611650</td>\n",
       "      <td>0.904880</td>\n",
       "      <td>0.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.218400</td>\n",
       "      <td>0.291808</td>\n",
       "      <td>0.648327</td>\n",
       "      <td>0.918697</td>\n",
       "      <td>0.458422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.302428</td>\n",
       "      <td>0.633150</td>\n",
       "      <td>0.909639</td>\n",
       "      <td>0.466951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.211100</td>\n",
       "      <td>0.310432</td>\n",
       "      <td>0.636637</td>\n",
       "      <td>0.910249</td>\n",
       "      <td>0.456290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.311749</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.914587</td>\n",
       "      <td>0.503198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.165200</td>\n",
       "      <td>0.332504</td>\n",
       "      <td>0.663345</td>\n",
       "      <td>0.909226</td>\n",
       "      <td>0.530917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.365211</td>\n",
       "      <td>0.672188</td>\n",
       "      <td>0.902746</td>\n",
       "      <td>0.535181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.129300</td>\n",
       "      <td>0.345727</td>\n",
       "      <td>0.686981</td>\n",
       "      <td>0.911243</td>\n",
       "      <td>0.528785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.349083</td>\n",
       "      <td>0.661932</td>\n",
       "      <td>0.910985</td>\n",
       "      <td>0.518124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.372503</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.903017</td>\n",
       "      <td>0.524520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>0.393460</td>\n",
       "      <td>0.674880</td>\n",
       "      <td>0.897148</td>\n",
       "      <td>0.518124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.901077</td>\n",
       "      <td>0.509595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.398896</td>\n",
       "      <td>0.683492</td>\n",
       "      <td>0.903806</td>\n",
       "      <td>0.533049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Santa fold 2/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4c91acb5d642b38fa73eafe40833d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cc182651f343fb99ebcbf4b4c0379f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1100' max='2360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1100/2360 13:12:55 < 15:09:54, 0.02 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.403300</td>\n",
       "      <td>0.400968</td>\n",
       "      <td>0.296952</td>\n",
       "      <td>0.824486</td>\n",
       "      <td>0.081023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.299075</td>\n",
       "      <td>0.557118</td>\n",
       "      <td>0.913764</td>\n",
       "      <td>0.360341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.273100</td>\n",
       "      <td>0.274050</td>\n",
       "      <td>0.671192</td>\n",
       "      <td>0.923032</td>\n",
       "      <td>0.460554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.281759</td>\n",
       "      <td>0.701876</td>\n",
       "      <td>0.921231</td>\n",
       "      <td>0.477612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.640700</td>\n",
       "      <td>0.921372</td>\n",
       "      <td>0.441365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.237700</td>\n",
       "      <td>0.268914</td>\n",
       "      <td>0.707825</td>\n",
       "      <td>0.926846</td>\n",
       "      <td>0.515991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.272180</td>\n",
       "      <td>0.649652</td>\n",
       "      <td>0.925455</td>\n",
       "      <td>0.464819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.275370</td>\n",
       "      <td>0.658499</td>\n",
       "      <td>0.924118</td>\n",
       "      <td>0.481876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.295738</td>\n",
       "      <td>0.680203</td>\n",
       "      <td>0.917320</td>\n",
       "      <td>0.520256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.183200</td>\n",
       "      <td>0.288113</td>\n",
       "      <td>0.702006</td>\n",
       "      <td>0.923271</td>\n",
       "      <td>0.565032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.282040</td>\n",
       "      <td>0.664653</td>\n",
       "      <td>0.922111</td>\n",
       "      <td>0.494670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Santa fold 3/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719917a5de384c00882d1cb174117b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb53ad3566b4570b101d526529b322c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1100' max='2360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1100/2360 2:17:06 < 2:37:20, 0.13 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.396777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836207</td>\n",
       "      <td>0.074786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>0.311506</td>\n",
       "      <td>0.432379</td>\n",
       "      <td>0.907619</td>\n",
       "      <td>0.292735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.275300</td>\n",
       "      <td>0.279322</td>\n",
       "      <td>0.669162</td>\n",
       "      <td>0.922881</td>\n",
       "      <td>0.448718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.281901</td>\n",
       "      <td>0.626794</td>\n",
       "      <td>0.920306</td>\n",
       "      <td>0.420940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.298049</td>\n",
       "      <td>0.627543</td>\n",
       "      <td>0.909520</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.278113</td>\n",
       "      <td>0.710035</td>\n",
       "      <td>0.927709</td>\n",
       "      <td>0.523504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.267333</td>\n",
       "      <td>0.688188</td>\n",
       "      <td>0.930581</td>\n",
       "      <td>0.508547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.233500</td>\n",
       "      <td>0.274551</td>\n",
       "      <td>0.646081</td>\n",
       "      <td>0.925681</td>\n",
       "      <td>0.463675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.278854</td>\n",
       "      <td>0.705801</td>\n",
       "      <td>0.934951</td>\n",
       "      <td>0.542735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.280669</td>\n",
       "      <td>0.700428</td>\n",
       "      <td>0.932178</td>\n",
       "      <td>0.534188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.285378</td>\n",
       "      <td>0.709633</td>\n",
       "      <td>0.932744</td>\n",
       "      <td>0.547009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Santa fold 4/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6598d240ac462395891d57b0914de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6038af94226f4f72980a793efb3f82d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1700' max='2360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1700/2360 3:33:27 < 1:22:58, 0.13 it/s, Epoch 14/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.384575</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.848815</td>\n",
       "      <td>0.079060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.313484</td>\n",
       "      <td>0.523050</td>\n",
       "      <td>0.899827</td>\n",
       "      <td>0.356838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.295600</td>\n",
       "      <td>0.301029</td>\n",
       "      <td>0.608340</td>\n",
       "      <td>0.905213</td>\n",
       "      <td>0.408120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.323100</td>\n",
       "      <td>0.302876</td>\n",
       "      <td>0.557940</td>\n",
       "      <td>0.899275</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.223500</td>\n",
       "      <td>0.292567</td>\n",
       "      <td>0.645802</td>\n",
       "      <td>0.911122</td>\n",
       "      <td>0.457265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.233000</td>\n",
       "      <td>0.293338</td>\n",
       "      <td>0.624214</td>\n",
       "      <td>0.914067</td>\n",
       "      <td>0.435897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.285210</td>\n",
       "      <td>0.593776</td>\n",
       "      <td>0.917565</td>\n",
       "      <td>0.403846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.293266</td>\n",
       "      <td>0.597164</td>\n",
       "      <td>0.918162</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.168600</td>\n",
       "      <td>0.314899</td>\n",
       "      <td>0.659957</td>\n",
       "      <td>0.913769</td>\n",
       "      <td>0.482906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>0.325064</td>\n",
       "      <td>0.628615</td>\n",
       "      <td>0.912044</td>\n",
       "      <td>0.474359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.146700</td>\n",
       "      <td>0.334113</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.912486</td>\n",
       "      <td>0.487179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.143900</td>\n",
       "      <td>0.371986</td>\n",
       "      <td>0.664850</td>\n",
       "      <td>0.908014</td>\n",
       "      <td>0.517094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>0.350052</td>\n",
       "      <td>0.639130</td>\n",
       "      <td>0.911399</td>\n",
       "      <td>0.480769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.361655</td>\n",
       "      <td>0.661613</td>\n",
       "      <td>0.912249</td>\n",
       "      <td>0.519231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.099200</td>\n",
       "      <td>0.352223</td>\n",
       "      <td>0.654232</td>\n",
       "      <td>0.914417</td>\n",
       "      <td>0.504274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.392106</td>\n",
       "      <td>0.655579</td>\n",
       "      <td>0.908942</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.389237</td>\n",
       "      <td>0.642806</td>\n",
       "      <td>0.906539</td>\n",
       "      <td>0.489316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Santa fold 5/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586f22309a8543688997db8cbcda9ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e8205504d04b38b9c7671e30f9bde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='2360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/2360 4:02:41 < 1:15:35, 0.12 it/s, Epoch 15/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.412602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.825189</td>\n",
       "      <td>0.066239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.307884</td>\n",
       "      <td>0.662816</td>\n",
       "      <td>0.916358</td>\n",
       "      <td>0.420940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.305888</td>\n",
       "      <td>0.500445</td>\n",
       "      <td>0.914091</td>\n",
       "      <td>0.335470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.279082</td>\n",
       "      <td>0.645358</td>\n",
       "      <td>0.923685</td>\n",
       "      <td>0.431624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>0.286276</td>\n",
       "      <td>0.666194</td>\n",
       "      <td>0.922303</td>\n",
       "      <td>0.476496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.286033</td>\n",
       "      <td>0.689170</td>\n",
       "      <td>0.923378</td>\n",
       "      <td>0.506410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.205700</td>\n",
       "      <td>0.285010</td>\n",
       "      <td>0.675381</td>\n",
       "      <td>0.923957</td>\n",
       "      <td>0.457265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.275880</td>\n",
       "      <td>0.639571</td>\n",
       "      <td>0.929339</td>\n",
       "      <td>0.448718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.286252</td>\n",
       "      <td>0.669123</td>\n",
       "      <td>0.925873</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.296052</td>\n",
       "      <td>0.680115</td>\n",
       "      <td>0.925567</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.133400</td>\n",
       "      <td>0.299976</td>\n",
       "      <td>0.698434</td>\n",
       "      <td>0.930371</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.314075</td>\n",
       "      <td>0.713419</td>\n",
       "      <td>0.927512</td>\n",
       "      <td>0.544872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.317971</td>\n",
       "      <td>0.714956</td>\n",
       "      <td>0.930507</td>\n",
       "      <td>0.542735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.321855</td>\n",
       "      <td>0.703804</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.523504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.335149</td>\n",
       "      <td>0.701706</td>\n",
       "      <td>0.928162</td>\n",
       "      <td>0.534188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.098500</td>\n",
       "      <td>0.361589</td>\n",
       "      <td>0.701942</td>\n",
       "      <td>0.921641</td>\n",
       "      <td>0.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.365316</td>\n",
       "      <td>0.702310</td>\n",
       "      <td>0.923443</td>\n",
       "      <td>0.529915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.089100</td>\n",
       "      <td>0.389363</td>\n",
       "      <td>0.696111</td>\n",
       "      <td>0.916189</td>\n",
       "      <td>0.534188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 01:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mystery fold 1/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1d442fd8c44382b368154009d8f0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c3264c44fc427aa36bca9bcde871ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 34:27, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>0.343149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.848003</td>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>0.285850</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.883171</td>\n",
       "      <td>0.465517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.330557</td>\n",
       "      <td>0.589928</td>\n",
       "      <td>0.842074</td>\n",
       "      <td>0.517241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mystery fold 2/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327cb7fd69a84ee68439fb85a551af4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d5b76536444085840a511c1148535b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 33:49, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>0.325412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.873683</td>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.238200</td>\n",
       "      <td>0.278525</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.910620</td>\n",
       "      <td>0.534483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.318561</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.884633</td>\n",
       "      <td>0.413793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mystery fold 3/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df826cb45e9c4f179d1f0d1048639ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/230 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed85fecd21da481a890d5413a67b2394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 34:00, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.344600</td>\n",
       "      <td>0.299403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888263</td>\n",
       "      <td>0.385965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.230300</td>\n",
       "      <td>0.322293</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>0.855103</td>\n",
       "      <td>0.561404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.377526</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.819439</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mystery fold 4/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdd1709fb6c4aad8f64d831a5e04072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/230 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122d81f9b89f4b8bbe92d2c008ca4d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 35:56, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.325100</td>\n",
       "      <td>0.335280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.858285</td>\n",
       "      <td>0.280702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.352364</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.841977</td>\n",
       "      <td>0.403509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.343962</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.877529</td>\n",
       "      <td>0.491228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mystery fold 5/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8687435736cc4601b13ce0c3e67004e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/230 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efede40ed0f43e8b44149bafdc9cad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 35:44, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>0.314106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.887909</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.281446</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.903128</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.367275</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>0.905908</td>\n",
       "      <td>0.491228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Combined fold 1/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3f97f30c9445ee91675562a8757ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ce45156cde4f62a247c7b5cf1dca85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [8] at entry 0 and [11] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 453\u001b[39m\n\u001b[32m    450\u001b[39m     summary_df.to_csv(\u001b[33m'\u001b[39m\u001b[33mcomparison_summary.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m     compare_models()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 413\u001b[39m, in \u001b[36mcompare_models\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# Perform k-fold for each model separately\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, folder_path \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     results[model_name] = train_with_kfold(model_name, folder_path, output_dir, k=\u001b[32m5\u001b[39m, epochs=\u001b[32m20\u001b[39m, batch_size=\u001b[32m16\u001b[39m)\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# Prepare Combined data for Ollama evaluation\u001b[39;00m\n\u001b[32m    416\u001b[39m all_texts, all_labels, num_labels, all_ids = [], [], \u001b[32m0\u001b[39m, []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mtrain_with_kfold\u001b[39m\u001b[34m(model_name, folder_path, output_dir, k, epochs, batch_size)\u001b[39m\n\u001b[32m    112\u001b[39m training_args = TrainingArguments(\n\u001b[32m    113\u001b[39m     output_dir=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    114\u001b[39m     num_train_epochs=epochs,\n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m     seed=SEED\n\u001b[32m    129\u001b[39m )\n\u001b[32m    131\u001b[39m trainer = Trainer(\n\u001b[32m    132\u001b[39m     model=model,\n\u001b[32m    133\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m     callbacks=[EarlyStoppingCallback(early_stopping_patience=\u001b[32m5\u001b[39m)]\n\u001b[32m    138\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m trainer.train()\n\u001b[32m    141\u001b[39m eval_results = trainer.evaluate()\n\u001b[32m    142\u001b[39m fold_results.append(eval_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\Roberta\\Lib\\site-packages\\transformers\\trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[32m   2239\u001b[39m         args=args,\n\u001b[32m   2240\u001b[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001b[32m   2241\u001b[39m         trial=trial,\n\u001b[32m   2242\u001b[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001b[32m   2243\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\Roberta\\Lib\\site-packages\\transformers\\trainer.py:2533\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2531\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2532\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2533\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28mself\u001b[39m.get_batch_samples(epoch_iterator, num_batches, args.device)\n\u001b[32m   2534\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2535\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2536\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\Roberta\\Lib\\site-packages\\transformers\\trainer.py:5355\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5353\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5354\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5355\u001b[39m         batch_samples.append(\u001b[38;5;28mnext\u001b[39m(epoch_iterator))\n\u001b[32m   5356\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5357\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\Roberta\\Lib\\site-packages\\accelerate\\data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\Roberta\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28mself\u001b[39m._next_data()\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\Roberta\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._dataset_fetcher.fetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\Roberta\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.collate_fn(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\Roberta\\Lib\\site-packages\\transformers\\data\\data_collator.py:93\u001b[39m, in \u001b[36mdefault_data_collator\u001b[39m\u001b[34m(features, return_tensors)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\Roberta\\Lib\\site-packages\\transformers\\data\\data_collator.py:155\u001b[39m, in \u001b[36mtorch_default_data_collator\u001b[39m\u001b[34m(features)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabel_ids\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         batch[k] = torch.stack([f[k] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features])\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np.ndarray):\n\u001b[32m    157\u001b[39m         batch[k] = torch.from_numpy(np.stack([f[k] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n",
      "\u001b[31mRuntimeError\u001b[39m: stack expects each tensor to be equal size, but got [8] at entry 0 and [11] at entry 3"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from datasets.features import Features, Value, Sequence\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from typing import List, Dict\n",
    "import ollama\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Unified data loading\n",
    "def load_data(csv_path: str) -> tuple[List[str], List[List[float]], int, List[str]]:\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding=encoding, delimiter=';', quotechar='\"', on_bad_lines='warn')\n",
    "            if not df.empty:\n",
    "                break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    else:\n",
    "        raise UnicodeDecodeError(f\"Failed to decode {csv_path} with tried encodings: {encodings}\")\n",
    "    \n",
    "    text_column = 'Text'\n",
    "    all_columns = df.columns.tolist()\n",
    "    if 'SANTA_ID' in all_columns:\n",
    "        ids = df['SANTA_ID'].tolist()\n",
    "        all_columns.remove('SANTA_ID')\n",
    "    else:\n",
    "        ids = [f\"ID_{i}\" for i in range(len(df))]\n",
    "    if text_column in all_columns:\n",
    "        all_columns.remove(text_column)\n",
    "    \n",
    "    # Remove the last 3 columns to match 11 labels\n",
    "    label_columns = all_columns[:-3]\n",
    "    \n",
    "    # Regularize label columns\n",
    "    df[label_columns] = df[label_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    \n",
    "    texts = df[text_column].tolist()\n",
    "    labels = df[label_columns].values.astype(float).tolist()\n",
    "    return texts, labels, len(label_columns), ids\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = (pred.predictions > 0.5).astype(float)\n",
    "    f1 = f1_score(labels, preds, average='micro')\n",
    "    roc_auc = roc_auc_score(labels, pred.predictions, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'f1': f1, 'roc_auc': roc_auc, 'accuracy': acc}\n",
    "\n",
    "# Function to perform k-fold training and evaluation\n",
    "def train_with_kfold(model_name: str, folder_path: str, output_dir: str, k: int = 5, epochs: int = 20, batch_size: int = 16) -> Dict:\n",
    "    all_texts, all_labels, num_labels, all_ids = [], [], 0, []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_path = os.path.join(folder_path, filename)\n",
    "            texts, labels, n_labels, ids = load_data(csv_path)\n",
    "            all_texts.extend(texts)\n",
    "            all_labels.extend(labels)\n",
    "            all_ids.extend(ids)\n",
    "            num_labels = n_labels  # Assume consistent number of labels\n",
    "    \n",
    "    if len(all_texts) < k:\n",
    "        raise ValueError(f\"Insufficient samples ({len(all_texts)}) for {k}-fold cross-validation\")\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(all_texts)):\n",
    "        print(f\"Training {model_name} fold {fold + 1}/{k}...\")\n",
    "        train_texts = [all_texts[i] for i in train_idx]\n",
    "        test_texts = [all_texts[i] for i in test_idx]\n",
    "        train_labels = [all_labels[i] for i in train_idx]\n",
    "        test_labels = [all_labels[i] for i in test_idx]\n",
    "        train_ids = [all_ids[i] for i in train_idx]\n",
    "        test_ids = [all_ids[i] for i in test_idx]\n",
    "        \n",
    "        features = Features({'text': Value('string'), 'labels': Sequence(Value('float32'))})\n",
    "        train_dataset_dict = {'text': train_texts, 'labels': train_labels}\n",
    "        test_dataset_dict = {'text': test_texts, 'labels': test_labels}\n",
    "        \n",
    "        train_dataset = Dataset.from_dict(train_dataset_dict, features=features)\n",
    "        test_dataset = Dataset.from_dict(test_dataset_dict, features=features)\n",
    "        \n",
    "        tokenizer = RobertaTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
    "        train_dataset = train_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "        test_dataset = test_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "        \n",
    "        train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            'FacebookAI/roberta-base', num_labels=num_labels, problem_type='multi_label_classification'\n",
    "        )\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{output_dir}/{model_name}_fold_{fold + 1}\",\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "            eval_strategy='steps',\n",
    "            eval_steps=100,\n",
    "            save_strategy='steps',\n",
    "            save_steps=100,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='f1',\n",
    "            seed=SEED\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        fold_results.append(eval_results)\n",
    "        trainer.save_model(f\"{output_dir}/{model_name}_fold_{fold + 1}\")\n",
    "        tokenizer.save_pretrained(f\"{output_dir}/{model_name}_fold_{fold + 1}\")\n",
    "    \n",
    "    avg_results = {k: np.mean([r[k] for r in fold_results]) for k in fold_results[0].keys() if k not in ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second']}\n",
    "    return avg_results\n",
    "\n",
    "# Ollama prediction function\n",
    "def get_ollama_predictions(test_texts: List[str], test_ids: List[str], num_labels: int) -> np.ndarray:\n",
    "    examples = [\n",
    "        \"\"\"ACI08;\"Mr. Hosmer Angel came to the house again and proposed that we \n",
    "should marry before father came back. He was in dreadful earnest \n",
    "and made me swear, with my hands on the Testament, that whatever \n",
    "happened I would always be true to him. Mother said he was quite \n",
    "right to make me swear, and that it was a sign of his passion. \n",
    "Mother was all in his favour from the first and was even fonder \n",
    "of him than I was. Then, when they talked of marrying within the \n",
    "week, I began to ask about father; but they both said never to \n",
    "mind about father, but just to tell him afterwards, and mother \n",
    "said she would make it all right with him.\";0;0;0;1;0;1;1;0;0;0;none;0;0\"\"\",\n",
    "        \"\"\"ACI05;\"I had had so many reasons to believe in my friend's subtle powers \n",
    "of reasoning and extraordinary energy in action that I felt that \n",
    "he must have some solid grounds for the assured and easy \n",
    "demeanour with which he treated the singular mystery which he had \n",
    "been called upon to fathom. Once only had I known him to fail, in \n",
    "the case of the King of Bohemia and of the Irene Adler \n",
    "photograph; but when I looked back to the weird business of the \n",
    "Sign of Four, and the extraordinary circumstances connected with \n",
    "the Study in Scarlet, I felt that it would be a strange tangle \n",
    "indeed which he could not unravel. \n",
    "I left him then, still puffing at his black clay pipe, with the \n",
    "conviction that when I came again on the next evening I would \n",
    "find that he held in his hands all the clues which would lead up \n",
    "to the identity of the disappearing bridegroom of Miss Mary \n",
    "Sutherland. \n",
    "A professional case of great gravity was engaging my own \n",
    "attention at the time, and the whole of next day I was busy at \n",
    "the bedside of the sufferer. It was not until close upon six \n",
    "o'clock that I found myself free and was able to spring into a \n",
    "hansom and drive to Baker Street, half afraid that I might be too \n",
    "late to assist at the dénouement of the little mystery. I found \n",
    "Sherlock Holmes alone, however, half asleep, with his long, thin \n",
    "form curled up in the recesses of his armchair. A formidable \n",
    "array of bottles and test-tubes, with the pungent cleanly smell \n",
    "of hydrochloric acid, told me that he had spent his day in the \n",
    "chemical work which was so dear to him.\";0;1;0;0;0;1;1;1;0;0;none;0;0\"\"\",\n",
    "        \"\"\"TAIN27;\"On the previous morning, two gentlemen had called to see his \n",
    "master. They were Italians, and the elder of the two, a man of about \n",
    "forty, gave his name as Signor Ascanio. \n",
    "The younger was a well-dressed lad of about twenty-four. \n",
    "Count Foscatini was evidently prepared for their visit and \n",
    "immediately sent Graves out upon some trivial errand. \n",
    "Here the man paused and hesitated in his story. \n",
    "In the end, however, he admitted that, curious as to the purport \n",
    "of the interview, he had not obeyed immediately, but had lingered \n",
    "about endeavouring to hear something of what was going on. \n",
    "The conversation was carried on in so low a tone that he was not \n",
    "as successful as he had hoped; but he gathered enough to make it \n",
    "clear that some kind of monetary proposition was being discussed, \n",
    "and that the basis of it was a threat. \n",
    "The discussion was anything but amicable. \n",
    "In the end, Count Foscatini raised his voice slightly, and the \n",
    "listener heard these words clearly:  'I have no time to argue \n",
    "further now, gentlemen. \n",
    "If you will dine with me to-morrow night at eight o’clock, we will\n",
    " resume the discussion.'  Afraid of being discovered listening, \n",
    "Graves had then hurried out to do his master’s errand. \n",
    "This evening the two men had arrived punctually at eight. \n",
    "During dinner they had talked of indifferent matters—politics, the\n",
    " weather, and the theatrical world. \n",
    "When Graves had placed the port upon the table and brought in the \n",
    "coffee his master told him that he might have the evening off. \n",
    "'Was that a usual proceeding of his when he had guests?' asked the\n",
    " inspector. \n",
    "'No, sir; it wasn’t. \n",
    "That’s what made me think it must be some business of a very \n",
    "unusual kind that he was going to discuss with these gentlemen.'  \n",
    "That finished Graves’s story. \n",
    "He had gone out about 8.30, and, meeting a friend, had accompanied\n",
    " him to the Metropolitan Music Hall in Edgware Road. \n",
    "Nobody had seen the two men leave, but the time of the murder was \n",
    "fixed clearly enough at 8.47. \n",
    "A small clock on the writing-table had been swept off by \n",
    "Foscatini’s arm, and had stopped at that hour, which agreed with \n",
    "Miss Rider’s telephone summons. \n",
    "The police surgeon had made his examination of the body, and it \n",
    "was now lying on the couch. \n",
    "I saw the face for the first time—the olive complexion, the long \n",
    "nose, the luxuriant black moustache, and the full red lips drawn \n",
    "back from the dazzlingly white teeth. \n",
    "Not altogether a pleasant face. \n",
    "'Well,' said the inspector, refastening his notebook. \n",
    "'The case seems clear enough. \n",
    "The only difficulty will be to lay our hands on this Signor \n",
    "Ascanio. \n",
    "I suppose his address is not in the dead man’s pocket-book by any \n",
    "chance?'  As Poirot had said, the late Foscatini was an orderly \n",
    "man. \n",
    "Neatly written in small, precise handwriting was the inscription, \n",
    "'Signor Paolo Ascanio, Grosvenor Hotel.'  The inspector busied \n",
    "himself with the telephone, then turned to us with a grin. \n",
    "'Just in time. \n",
    "Our fine gentleman was off to catch the boat train to the \n",
    "Continong. \n",
    "Well, gentlemen, that’s about all we can do here. \n",
    "It’s a bad business, but straightforward enough. \n",
    "One of these Italian vendetta things, as likely as not.'  Thus \n",
    "airily dismissed, we found our way downstairs. \n",
    "Dr. Hawker was full of excitement. \n",
    "'Like the beginning of a novel, eh? \n",
    "Real exciting stuff. \n",
    "Wouldn’t believe it if you read about it.'  Poirot did not speak. \n",
    "He was very thoughtful. \n",
    "All the evening he had hardly opened his lips. \n",
    "'What says the master detective, eh?' asked Hawker, clapping him \n",
    "on the back. \n",
    "'Nothing to work your grey cells over this time.'  'You think \n",
    "not?'  'What could there be?'  'Well, for example, there is the \n",
    "window.'  'The window? \n",
    "But it was fastened. \n",
    "Nobody could have got out or in that way. \n",
    "I noticed it specially.'  'And why were you able to notice it?'  \n",
    "The doctor looked puzzled. \n",
    "Poirot hastened to explain. \n",
    "'It is to the curtains I refer. \n",
    "They were not drawn. \n",
    "A little odd, that. \n",
    "And then there was the coffee. \n",
    "It was very black coffee.'  'Well, what of it?'  'Very black,' \n",
    "repeated Poirot. \n",
    "'In conjunction with that let us remember that very little of the \n",
    "rice soufflé was eaten, and we get—what?'  'Moonshine,' laughed \n",
    "the doctor. \n",
    "'You’re pulling my leg.'  'Never do I pull the leg. \n",
    "Hastings here knows that I am perfectly serious.'  'I don’t know \n",
    "what you are getting at, all the same,' I confessed. \n",
    "'You don’t suspect the manservant, do you? \n",
    "He might have been in with the gang, and put some dope in the \n",
    "coffee. \n",
    "I suppose they’ll test his alibi?'  'Without doubt, my friend; but\n",
    " it is the alibi of Signor Ascanio that interests me.'  'You think\n",
    " he has an alibi?'  'That is just what worries me. \n",
    "I have no doubt that we will soon be enlightened on that point.'\n",
    " The _Daily Newsmonger_ enabled us to become conversant with \n",
    "succeeding events. \n",
    "Signor Ascanio was arrested and charged with the murder of Count \n",
    "Foscatini. \n",
    "When arrested, he denied knowing the Count, and declared he had \n",
    "never been near Regent’s Court either on the evening of the crime \n",
    "or on the previous morning. \n",
    "The younger man had disappeared entirely. \n",
    "Signor Ascanio had arrived alone at the Grosvenor Hotel from the \n",
    "Continent two days before the murder. \n",
    "All efforts to trace the second man failed. \n",
    "Ascanio, however, was not sent for trial. \n",
    "No less a personage than the Italian Ambassador himself came \n",
    "forward and testified at the police-court proceedings that Ascanio\n",
    " had been with him at the Embassy from eight till nine that \n",
    "evening. \n",
    "The prisoner was discharged. \n",
    "Naturally, a lot of people thought that the crime was a political \n",
    "one, and was being deliberately hushed up. \n",
    "Poirot had taken a keen interest in all these points. \n",
    "Nevertheless, I was somewhat surprised when he suddenly informed \n",
    "me one morning that he was expecting a visitor at eleven o’clock, \n",
    "and that that visitor was none other than Ascanio himself. \n",
    "'He wishes to consult you?'  '_Du tout_, Hastings. \n",
    "I wish to consult him.'  'What about?'  'The Regent’s Court \n",
    "murder.'  'You are going to prove that he did it?'  'A man cannot \n",
    "be tried twice for murder, Hastings. \n",
    "Endeavour to have the common sense. \n",
    "Ah, that is our friend’s ring. A few minutes later Signor \n",
    "Ascanio was ushered in—a small, thin man with a secretive and \n",
    "furtive glance in his eyes. \n",
    "He remained standing, darting suspicious glances from one to the \n",
    "other of us. \n",
    "'Monsieur Poirot?'  My little friend tapped himself gently on the \n",
    "chest. \n",
    "'Be seated, signor. \n",
    "You received my note. \n",
    "I am determined to get to the bottom of this mystery. \n",
    "In some small measure you can aid me. \n",
    "Let us commence. \n",
    "You—in company with a friend—visited the late Count Foscatini on \n",
    "the morning of Tuesday the 9th——'  The Italian made an angry \n",
    "gesture. \n",
    "'I did nothing of the sort. \n",
    "I have sworn in court——'  '_Précisément_—and I have a little idea \n",
    "that you have sworn falsely.'  'You threaten me? \n",
    "Bah! \n",
    "I have nothing to fear from you. \n",
    "I have been acquitted.'  'Exactly; and as I am not an imbecile, it\n",
    " is not with the gallows I threaten you—but with publicity. \n",
    "Publicity! \n",
    "I see that you do not like the word. \n",
    "I had an idea that you would not. \n",
    "My little ideas, you know, they are very valuable to me. \n",
    "Come, signor, your only chance is to be frank with me. \n",
    "I do not ask to know whose indiscretions brought you to England. \n",
    "I know this much, you came for the especial purpose of seeing \n",
    "Count Foscatini.'  'He was not a count,' growled the Italian. \n",
    "'I have already noted the fact that his name does not appear in \n",
    "the _Almanach de Gotha_. \n",
    "Never mind, the title of count is often useful in the profession \n",
    "of blackmailing.'  'I suppose I might as well be frank. \n",
    "You seem to know a good deal.'  'I have employed my grey cells to \n",
    "some advantage. \n",
    "Come, Signor Ascanio, you visited the dead man on the Tuesday \n",
    "morning—that is so, is it not?'  'Yes; but I never went there on \n",
    "the following evening. \n",
    "There was no need. \n",
    "I will tell you all.';0;0;0;0;0;0;1;0;0;0;none;0;0\"\"\"\n",
    "    ]\n",
    "    examples_str = \"\\n\".join(examples)\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "You are an expert in narratology, annotating literary texts based on the modular guidelines from Heyns and Van Zaanen (2024) for mystery novels (whodunits). For each text segment, output a CSV row with these columns: SANTA_ID;Text;Scene;Summary;Descriptive_passage;Analepsis;Prolepsis;Extradiegetic;Intradiegetic;Metadiegetic;Focalization;Voice_homodiegetic;Voice_heterodiegetic.\n",
    "Here are the definitions for each tag to guide your annotation:\n",
    "\n",
    "Scene: A segment of narrative discourse that presents the histoire (story), typically involving a coherent sequence of events with specific characters, time, and place, annotated using the SCENE tag.\n",
    "Summary: A non-scene where events are condensed or narrated briefly, often as a sub-scene within a broader scene, assigned as a property of NON-SCENE.\n",
    "Descriptive_passage: A non-scene focused on description rather than events, providing detailed pauses in the narrative for setting or character details, assigned as a property of NON-SCENE.\n",
    "Analepsis: A flashback that shifts the narrative time from the present to the past, tagged as ANALEPSIS, which can be embedded or interruptive.\n",
    "Prolepsis: A flash-forward occurring in forms like visions, prophecies, or foreshadowing, shifting narrative time to the future, tagged as PROLEPSIS, which can be embedded or interruptive.\n",
    "Extradiegetic: The level of the narrator or implied author outside the story world, annotated with NARRATOR and value 0, potentially including metatextuality (meta).\n",
    "Intradiegetic: The diegetic level of characters and events within the story, annotated with value 1 (and letters like 1a, 1b for sequential arrangement).\n",
    "Metadiegetic: A secondary narrative embedded within the primary diegetic level, such as stories told by characters, annotated with value 2 (and letters for arrangement).\n",
    "Focalization: The perspective from which the narrative is seen, indicating the narrator's access to information; can be zero/unrestricted (omniscient, knows more than characters), internal (limited to a character's knowledge), or external (observes without internal access); tag with FOCALIZATION and properties like EMBEDDED or INTERRUPTIVE.\n",
    "Voice_homodiegetic: When the narrator appears in the story as a character, usually referring to themselves in the first person.\n",
    "Voice_heterodiegetic: When the narrator does not appear in the story, with narration mostly in the third person.\n",
    "\n",
    "Use 1 for 'yes' and 0 for 'no' in binary columns (e.g., Scene). \n",
    "Examples:\n",
    "{examples_str}\n",
    "\n",
    "Now annotate this new segment:\n",
    "ID: {id}\n",
    "Text: {text}\n",
    "Output only the CSV row (no extra text).\n",
    "\"\"\"\n",
    "\n",
    "    annotations = []\n",
    "    start_time = time.time()\n",
    "    for text, id_val in zip(test_texts, test_ids):\n",
    "        print(f\"Processing row for ID: {id_val}\")\n",
    "        prompt = prompt_template.format(id=id_val, text=text)\n",
    "        response = ollama.chat(model='llama3:8b', messages=[{'role': 'user', 'content': prompt}], options={'temperature': 0})\n",
    "        annotated_row = response['message']['content'].strip()\n",
    "        annotations.append(annotated_row)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Parse annotations into a structured format for metrics\n",
    "    pred_labels = []\n",
    "    for row in annotations:\n",
    "        parts = row.split(';')[2:]  # Skip SANTA_ID and Text\n",
    "        pred_labels.append([float(x) for x in parts])\n",
    "    return np.array(pred_labels)\n",
    "\n",
    "def compare_models():\n",
    "    models = {\n",
    "        'Santa': 'Finaal_data\\\\SANTA',\n",
    "        'Mystery': 'Finaal_data\\\\MD',\n",
    "        'Combined': 'Finaal_data\\\\Combination'\n",
    "    }\n",
    "    output_dir = './fine_tuned_models'\n",
    "    results = {}\n",
    "    \n",
    "    # Perform k-fold for each model separately\n",
    "    for model_name, folder_path in models.items():\n",
    "        results[model_name] = train_with_kfold(model_name, folder_path, output_dir, k=5, epochs=20, batch_size=16)\n",
    "    \n",
    "    # Prepare Combined data for Ollama evaluation\n",
    "    all_texts, all_labels, num_labels, all_ids = [], [], 0, []\n",
    "    folder_path = models['Combined']\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_path = os.path.join(folder_path, filename)\n",
    "            texts, labels, n_labels, ids = load_data(csv_path)\n",
    "            all_texts.extend(texts)\n",
    "            all_labels.extend(labels)\n",
    "            all_ids.extend(ids)\n",
    "            num_labels = n_labels  # Assume consistent number of labels\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    ollama_fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(all_texts)):\n",
    "        print(f\"Evaluating Ollama fold {fold + 1}/5...\")\n",
    "        test_texts = [all_texts[i] for i in test_idx]\n",
    "        test_labels = [all_labels[i] for i in test_idx]\n",
    "        test_ids = [all_ids[i] for i in test_idx]\n",
    "        \n",
    "        ollama_preds = get_ollama_predictions(test_texts, test_ids, num_labels)\n",
    "        true_labels = np.array(test_labels)\n",
    "        f1 = f1_score(true_labels, ollama_preds, average='micro')\n",
    "        roc_auc = roc_auc_score(true_labels, ollama_preds, average='micro')\n",
    "        acc = accuracy_score(true_labels, ollama_preds)\n",
    "        ollama_fold_results.append({'f1': f1, 'roc_auc': roc_auc, 'accuracy': acc})\n",
    "    \n",
    "    avg_ollama_results = {k: np.mean([r[k] for r in ollama_fold_results]) for k in ollama_fold_results[0].keys()}\n",
    "    results['Ollama'] = avg_ollama_results\n",
    "    \n",
    "    # Summary table\n",
    "    summary_df = pd.DataFrame(results).T.round(4)\n",
    "    print(\"\\nOverall Metrics Summary:\")\n",
    "    print(summary_df)\n",
    "    summary_df.to_csv('comparison_summary.csv', index=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3590d015-a167-4006-9bc9-182d82082d5b",
   "metadata": {},
   "source": [
    "Train slegs combined en few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d3394-8b4f-4794-a661-6717740ccc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
